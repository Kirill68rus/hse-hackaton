{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95ef71e1",
   "metadata": {
    "cellId": "opdtmf2zaf9h1uwa7y9q6m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/jupyter/.local/lib/python3.8/site-packages (0.1.97)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50095a76",
   "metadata": {
    "cellId": "h7z1sf2ltfjil7umqhbcz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/jupyter/.local/lib/python3.8/site-packages (4.27.4)\n",
      "Requirement already satisfied: datasets in /home/jupyter/.local/lib/python3.8/site-packages (2.11.0)\n",
      "Requirement already satisfied: accelerate in /home/jupyter/.local/lib/python3.8/site-packages (0.18.0)\n",
      "Requirement already satisfied: deepspeed in /home/jupyter/.local/lib/python3.8/site-packages (0.8.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /kernel/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/jupyter/.local/lib/python3.8/site-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/jupyter/.local/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jupyter/.local/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: requests in /kernel/lib/python3.8/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/jupyter/.local/lib/python3.8/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (0.25.3)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.10.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2021.11.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/jupyter/.local/lib/python3.8/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: xxhash in /kernel/fallback/lib/python3.8/site-packages (from datasets) (2.0.0)\n",
      "Requirement already satisfied: multiprocess in /home/jupyter/.local/lib/python3.8/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.9.1+cu111)\n",
      "Requirement already satisfied: psutil in /kernel/lib/python3.8/site-packages (from accelerate) (5.7.3)\n",
      "Requirement already satisfied: py-cpuinfo in /home/jupyter/.local/lib/python3.8/site-packages (from deepspeed) (9.0.0)\n",
      "Requirement already satisfied: hjson in /home/jupyter/.local/lib/python3.8/site-packages (from deepspeed) (3.1.0)\n",
      "Requirement already satisfied: ninja in /home/jupyter/.local/lib/python3.8/site-packages (from deepspeed) (1.11.1)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.8/dist-packages (from deepspeed) (1.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /kernel/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /kernel/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /kernel/lib/python3.8/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /kernel/lib/python3.8/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: six in /kernel/lib/python3.8/site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /kernel/lib/python3.8/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (5.2.0)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /kernel/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Installing collected packages: charset-normalizer\n",
      "\u001b[33m  WARNING: The script normalizer is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "moto 1.3.14 requires idna<2.9,>=2.5, but you have idna 3.4 which is incompatible.\n",
      "kaggle 1.5.8 requires urllib3<1.25,>=1.21.1, but you have urllib3 1.26.15 which is incompatible.\n",
      "cloud-ml 0.0.1 requires requests<=2.25.1,>=2.22.0, but you have requests 2.28.2 which is incompatible.\n",
      "cloud-ml 0.0.1 requires tqdm<=4.54.1,>=4.45.0, but you have tqdm 4.65.0 which is incompatible.\u001b[0m\n",
      "Successfully installed charset-normalizer-2.1.1\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "\n",
    "\n",
    "%pip install --upgrade transformers datasets accelerate deepspeed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "# import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e20d2d1a",
   "metadata": {
    "cellId": "w7w83h7x9jml6oaewebuwc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9baafdcf29f48a98e85e136fa6cb7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)lve/main/config.json'), FloatProgress(value=0.0, max=674.0), HTML(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a56357176bf49f99f8ba7771b424e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)olve/main/vocab.json'), FloatProgress(value=0.0, max=1811625.0), HTM…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c9ca608b26410b89e6641a9bf55503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading (…)olve/main/merges.txt'), FloatProgress(value=0.0, max=1369443.0), HTM…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ai-forever/ruRoberta-large\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "492e7b62",
   "metadata": {
    "cellId": "dum67ndv14m0i5948u4us"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/jupyter/mnt/s3/dataset_hackaton/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "087847e6",
   "metadata": {
    "cellId": "q8wvy5j6u4phvs7f9wtyd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank-categories.csv\n",
      "bank-sentiment.csv\n",
      "banks-ethic-DB - NEW.docx\n",
      "homework_shad_da.ipynb.txt\n",
      "sentiment.csv\n",
      "train.csv\n"
     ]
    }
   ],
   "source": [
    "!ls /home/jupyter/mnt/s3/dataset_hackaton/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ec2eece",
   "metadata": {
    "cellId": "51f4ljuqgpn27aspwi5idn"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "\n",
    "bank_categories_df = pd.read_csv(DATA_PATH + 'bank-sentiment.csv')\n",
    "bank_sentiment_df = pd.read_csv(DATA_PATH + 'bank-categories.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aef01772",
   "metadata": {
    "cellId": "r9klh89kcmqd0c55qefgt"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence</th>\n",
       "      <th>1category</th>\n",
       "      <th>2category</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4754</td>\n",
       "      <td>При этом всегда получал качественные услуги.</td>\n",
       "      <td>Communication</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4417</td>\n",
       "      <td>Не вижу, за что хотя бы 2 поставить, сервис на 1!</td>\n",
       "      <td>?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3629</td>\n",
       "      <td>Вот так \"Мой любимый\" банк МКБ меня обманул.</td>\n",
       "      <td>?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11640</td>\n",
       "      <td>Отвратительное отношение к клиентам.</td>\n",
       "      <td>Communication</td>\n",
       "      <td>NaN</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5571</td>\n",
       "      <td>Всегда в любое время дня и ночи помогут, ответ...</td>\n",
       "      <td>Communication</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ... sentiment\n",
       "0        4754  ...         +\n",
       "1        4417  ...         −\n",
       "2        3629  ...         −\n",
       "3       11640  ...         −\n",
       "4        5571  ...         +\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff512458",
   "metadata": {
    "cellId": "wqwlhc2kuf4vd6xnd71oi"
   },
   "outputs": [],
   "source": [
    "# indexes = np.arange(train_df.shape[0])\n",
    "# np.random.shuffle(indexes)\n",
    "# train_size = math.ceil(train_df.shape[0] * 0.8)\n",
    "\n",
    "# train_indexes = indexes[:train_size]\n",
    "# test_indexes = indexes[train_size:]\n",
    "\n",
    "# train_data = train_df.iloc[train_indexes]\n",
    "# test_data = train_df.iloc[test_indexes]\n",
    "\n",
    "# train_data.reset_index(drop=True, inplace=True)\n",
    "# test_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "indexes = np.arange(train_df.shape[0])\n",
    "np.random.shuffle(indexes)\n",
    "train_size = math.ceil(train_df.shape[0] * 0.8)\n",
    "\n",
    "data = train_df.iloc[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e48e2eb6",
   "metadata": {
    "cellId": "0a0ym8ztqhvcisui5uoyvad"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.data = data\n",
    "\n",
    "        self.feature_column = feature_column\n",
    "        self.target_column = target_column\n",
    "        self.label2num = lambda x: x == '+'\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.data[self.feature_column][item]\n",
    "        label = self.label2num(self.data[self.target_column][item])\n",
    "\n",
    "        tokens = self.get_tokens_(text)\n",
    "\n",
    "        return {\"tokens\": tokens, \"target\": label}\n",
    "\n",
    "    def get_tokens_(self, text):\n",
    "          return self.tokenizer(text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4975251",
   "metadata": {
    "cellId": "604b78qb7vmd5ry63os8dn"
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(data, 'sentence', 'sentiment', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "568f0744",
   "metadata": {
    "cellId": "sfn8i5le0y0jbjveaqt8j8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Dataset at 0x7fabe61a4af0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d39f1849",
   "metadata": {
    "cellId": "k76d6nl292401u5jjfy4e"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "num_workers = 2\n",
    "\n",
    "def average_emb(batch):\n",
    "    features = [b[\"tokens\"] for b in batch]\n",
    "    targets = [b[\"target\"] for b in batch]\n",
    "\n",
    "    return {\"features\": features, \"targets\": torch.LongTensor(targets)}\n",
    "\n",
    "\n",
    "train_size = math.ceil(len(dataset) * 0.8)\n",
    "\n",
    "# train, valid = random_split(dataset, [train_size, len(train_data) - train_size])\n",
    "train, valid = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True, collate_fn=average_emb)\n",
    "valid_loader = DataLoader(valid, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, collate_fn=average_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e32ed8",
   "metadata": {
    "cellId": "ov9qsjdr3k29zvvssgs8o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d85bb0b4",
   "metadata": {
    "cellId": "88a3a283ogcpku4y3wcvj"
   },
   "outputs": [],
   "source": [
    "# next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59173cd3",
   "metadata": {
    "cellId": "7z0hawex60ps3relyatmt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d21220b",
   "metadata": {
    "cellId": "j0rb82g88yifoekop13wwh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313720b1",
   "metadata": {
    "cellId": "365yuv1swd97r4npr95mg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fda1b75",
   "metadata": {
    "cellId": "epltmkjgtbbsoya9zmyx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54631c6d",
   "metadata": {
    "cellId": "de284nye2lq63goywvlf4i"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8165da",
   "metadata": {
    "cellId": "5ctsv6y302g8dcybk3qj2s"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "b4080bcd-8e63-4852-b615-1487758e00d2",
  "notebookPath": "hse-hackaton/baseline_model/Untitled.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
