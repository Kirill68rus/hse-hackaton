{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "101fd8c0",
   "metadata": {
    "cellId": "opdtmf2zaf9h1uwa7y9q6m",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentencepiece in /home/jupyter/.local/lib/python3.8/site-packages (0.1.97)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "454cc13f",
   "metadata": {
    "cellId": "h7z1sf2ltfjil7umqhbcz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/jupyter/.local/lib/python3.8/site-packages (4.27.4)\n",
      "Requirement already satisfied: datasets in /home/jupyter/.local/lib/python3.8/site-packages (2.11.0)\n",
      "Requirement already satisfied: accelerate in /home/jupyter/.local/lib/python3.8/site-packages (0.18.0)\n",
      "Requirement already satisfied: deepspeed in /home/jupyter/.local/lib/python3.8/site-packages (0.8.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jupyter/.local/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: requests in /kernel/lib/python3.8/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /kernel/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/jupyter/.local/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/jupyter/.local/lib/python3.8/site-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: multiprocess in /home/jupyter/.local/lib/python3.8/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: xxhash in /kernel/fallback/lib/python3.8/site-packages (from datasets) (2.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (0.25.3)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.10.15)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/jupyter/.local/lib/python3.8/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2021.11.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/jupyter/.local/lib/python3.8/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: psutil in /kernel/lib/python3.8/site-packages (from accelerate) (5.7.3)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.9.1+cu111)\n",
      "Requirement already satisfied: py-cpuinfo in /home/jupyter/.local/lib/python3.8/site-packages (from deepspeed) (9.0.0)\n",
      "Requirement already satisfied: ninja in /home/jupyter/.local/lib/python3.8/site-packages (from deepspeed) (1.11.1)\n",
      "Requirement already satisfied: hjson in /home/jupyter/.local/lib/python3.8/site-packages (from deepspeed) (3.1.0)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.8/dist-packages (from deepspeed) (1.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /kernel/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /kernel/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /kernel/lib/python3.8/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /kernel/lib/python3.8/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: six in /kernel/lib/python3.8/site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /kernel/lib/python3.8/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (5.2.0)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /kernel/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2021.3)\n",
      "Installing collected packages: charset-normalizer\n",
      "\u001b[33m  WARNING: The script normalizer is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "moto 1.3.14 requires idna<2.9,>=2.5, but you have idna 3.4 which is incompatible.\n",
      "kaggle 1.5.8 requires urllib3<1.25,>=1.21.1, but you have urllib3 1.26.15 which is incompatible.\n",
      "cloud-ml 0.0.1 requires requests<=2.25.1,>=2.22.0, but you have requests 2.28.2 which is incompatible.\n",
      "cloud-ml 0.0.1 requires tqdm<=4.54.1,>=4.45.0, but you have tqdm 4.65.0 which is incompatible.\u001b[0m\n",
      "Successfully installed charset-normalizer-2.1.1\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "\n",
    "\n",
    "%pip install --upgrade transformers datasets accelerate deepspeed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "# import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bbe4cc",
   "metadata": {
    "cellId": "t59zouke3si9n4c7phledg",
    "execution_id": "10418c24-74d2-4963-8ce3-48fedffa937a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "430488dc",
   "metadata": {
    "cellId": "w7w83h7x9jml6oaewebuwc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22854f34a884e90846372136a5cd01c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/674 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89d8b0e281049f1bc7087e3305ea6ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.81M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58cbb2a75ce04737b4a8720059ab1978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/1.37M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89cef60c976b43fc83a2de3157f7aedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ai-forever/ruRoberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ai-forever/ruRoberta-large\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "247e80eb",
   "metadata": {
    "cellId": "zkiotc5u66g7j11lpt6l"
   },
   "outputs": [],
   "source": [
    "#!c1.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "595e9db3",
   "metadata": {
    "cellId": "dum67ndv14m0i5948u4us"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/jupyter/mnt/s3/dataset_hackaton/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4dc44aff",
   "metadata": {
    "cellId": "q8wvy5j6u4phvs7f9wtyd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank-categories.csv\n",
      "bank-sentiment.csv\n",
      "banks-ethic-DB - NEW.docx\n",
      "homework_shad_da.ipynb.txt\n",
      "sentiment.csv\n",
      "train.csv\n"
     ]
    }
   ],
   "source": [
    "!ls /home/jupyter/mnt/s3/dataset_hackaton/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2a8c9845",
   "metadata": {
    "cellId": "51f4ljuqgpn27aspwi5idn"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "\n",
    "bank_categories_df = pd.read_csv(DATA_PATH + 'bank-sentiment.csv')\n",
    "bank_sentiment_df = pd.read_csv(DATA_PATH + 'bank-categories.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4912ddd",
   "metadata": {
    "cellId": "r9klh89kcmqd0c55qefgt"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence</th>\n",
       "      <th>1category</th>\n",
       "      <th>2category</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4754</td>\n",
       "      <td>При этом всегда получал качественные услуги.</td>\n",
       "      <td>Communication</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4417</td>\n",
       "      <td>Не вижу, за что хотя бы 2 поставить, сервис на 1!</td>\n",
       "      <td>?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3629</td>\n",
       "      <td>Вот так \"Мой любимый\" банк МКБ меня обманул.</td>\n",
       "      <td>?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11640</td>\n",
       "      <td>Отвратительное отношение к клиентам.</td>\n",
       "      <td>Communication</td>\n",
       "      <td>NaN</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5571</td>\n",
       "      <td>Всегда в любое время дня и ночи помогут, ответ...</td>\n",
       "      <td>Communication</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19356</th>\n",
       "      <td>8004</td>\n",
       "      <td>Никогда и ни в коем случае не открывайте счет ...</td>\n",
       "      <td>Communication</td>\n",
       "      <td>NaN</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19357</th>\n",
       "      <td>18182</td>\n",
       "      <td>ТИ откровенно забили на качество и развивают с...</td>\n",
       "      <td>Quality</td>\n",
       "      <td>NaN</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19358</th>\n",
       "      <td>744</td>\n",
       "      <td>Я считаю, это прорыв и лидерство финансовых ус...</td>\n",
       "      <td>?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19359</th>\n",
       "      <td>6220</td>\n",
       "      <td>Писал мужчина очень доходчиво, не финансовым я...</td>\n",
       "      <td>Communication</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19360</th>\n",
       "      <td>8433</td>\n",
       "      <td>Данная ситуация меня сильно выбила из колеи, и...</td>\n",
       "      <td>Communication</td>\n",
       "      <td>NaN</td>\n",
       "      <td>−</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19361 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  ... sentiment\n",
       "0            4754  ...         +\n",
       "1            4417  ...         −\n",
       "2            3629  ...         −\n",
       "3           11640  ...         −\n",
       "4            5571  ...         +\n",
       "...           ...  ...       ...\n",
       "19356        8004  ...         −\n",
       "19357       18182  ...         −\n",
       "19358         744  ...         +\n",
       "19359        6220  ...         +\n",
       "19360        8433  ...         −\n",
       "\n",
       "[19361 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8622379e",
   "metadata": {
    "cellId": "wqwlhc2kuf4vd6xnd71oi"
   },
   "outputs": [],
   "source": [
    "# indexes = np.arange(train_df.shape[0])\n",
    "# np.random.shuffle(indexes)\n",
    "# train_size = math.ceil(train_df.shape[0] * 0.8)\n",
    "\n",
    "# train_indexes = indexes[:train_size]\n",
    "# test_indexes = indexes[train_size:]\n",
    "\n",
    "# train_data = train_df.iloc[train_indexes]\n",
    "# test_data = train_df.iloc[test_indexes]\n",
    "\n",
    "# train_data.reset_index(drop=True, inplace=True)\n",
    "# test_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "indexes = np.arange(train_df.shape[0])\n",
    "np.random.shuffle(indexes)\n",
    "train_size = math.ceil(train_df.shape[0] * 0.8)\n",
    "\n",
    "data = train_df.iloc[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9c224ca2",
   "metadata": {
    "cellId": "mlbsdz52hltrjj3i5pcl7"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 128\n",
    "def preprocess_function(examples):\n",
    "    result = tokenizer(\n",
    "        examples,\n",
    "        padding='max_length', max_length=MAX_LENGTH, truncation=True\n",
    "    )\n",
    "#     result['target'] = examples['target']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "42ff61fe",
   "metadata": {
    "cellId": "0a0ym8ztqhvcisui5uoyvad"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: list, tokenizer):  \n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.feature_column = feature_column\n",
    "        self.target_column = target_column\n",
    "        self.label2num = lambda x: x == '+'\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        indx_tokens = preprocess_function(self.data[self.feature_column][item])\n",
    "        label = self.label2num(self.data[self.target_column][item])\n",
    "        \n",
    "        return {**indx_tokens, \"target\": label}\n",
    "\n",
    "    def get_tokens_(self, text):\n",
    "          return self.tokenizer(text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "abcf8681",
   "metadata": {
    "cellId": "604b78qb7vmd5ry63os8dn"
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(data, 'sentence', 'sentiment', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ee24f95d",
   "metadata": {
    "cellId": "mw9252evmkfch53iypnn2k"
   },
   "outputs": [],
   "source": [
    "# dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8bc5b765",
   "metadata": {
    "cellId": "6gt8va8g32w14czuoh4lord"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7003c3f2",
   "metadata": {
    "cellId": "5ygm6d5oixklq9aoraiqvk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "deedf9dd",
   "metadata": {
    "cellId": "9b8yojfk3rjzw952th3c9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fbdaf4df",
   "metadata": {
    "cellId": "k76d6nl292401u5jjfy4e"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "num_workers = 4\n",
    "\n",
    "def average_emb(batch):\n",
    "    features = [b[\"tokens\"] for b in batch]\n",
    "    targets = [b[\"target\"] for b in batch]\n",
    "\n",
    "    return {\"tokens\": features, \"targets\": torch.LongTensor(targets)}\n",
    "\n",
    "\n",
    "train_size = math.ceil(len(dataset) * 0.8)\n",
    "\n",
    "# train, valid = random_split(dataset, [train_size, len(train_data) - train_size])\n",
    "train, valid = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True, collate_fn=transformers.default_data_collator)\n",
    "valid_loader = DataLoader(valid, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, collate_fn=transformers.default_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a25ef4e9",
   "metadata": {
    "cellId": "jt4je638agbtlclo8ttvj"
   },
   "outputs": [],
   "source": [
    "# next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4a6a0145",
   "metadata": {
    "cellId": "uzv81u85e6nltt7e48ljxh"
   },
   "outputs": [],
   "source": [
    "newmodel = torch.nn.Sequential(*(list(model.children())[:-1]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918688f3",
   "metadata": {
    "cellId": "74zvmn81tvuupe5aatkee8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83d3140f",
   "metadata": {
    "cellId": "rseu4nzvpzr71mlrwmiw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cad3c521",
   "metadata": {
    "cellId": "j7yw69bx4q8dxdbk7bb5di"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 47756, 15780,  ...,     0,     0,     0],\n",
       "         [    1,   608,  6661,  ...,     0,     0,     0],\n",
       "         [    1, 46718,  6722,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [    1,  1229,  1058,  ...,     0,     0,     0],\n",
       "         [    1, 35882,  2364,  ...,     0,     0,     0],\n",
       "         [    1, 19951,    16,  ...,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'target': tensor([ True, False, False,  True, False,  True,  True, False])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1160fa6c",
   "metadata": {
    "cellId": "sb76nxo0unnmwa8sor5s"
   },
   "outputs": [],
   "source": [
    "# newmodel.forward(input_ids=batch['input_ids'], attention_mask=batch['attention_mask']).last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cf0d1d85",
   "metadata": {
    "cellId": "h52nxu5hdbksfx2pb4zb2b"
   },
   "outputs": [],
   "source": [
    "def freeze_layers(model, num_to_freeze=1):\n",
    "    \n",
    "    for layer in list(model.parameters())[:-num_to_freeze]:\n",
    "        layer.requires_grad_ = False\n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9add30c2",
   "metadata": {
    "cellId": "p8kpzjyji7fgdaawr35705"
   },
   "outputs": [],
   "source": [
    "class FBERTAcls(nn.Module):\n",
    "\n",
    "    def __init__(self, model, last_layer_dim=768, hidden_layer_size=256):\n",
    "        super().__init__()\n",
    "        self.BERT = freeze_layers(model, 0)\n",
    "        self.hidden_layer_1 = nn.Linear(last_layer_dim, hidden_layer_size)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.bn_1 = nn.BatchNorm1d(hidden_layer_size)\n",
    "        self.hidden_layer_2 = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        self.bn_2 = nn.BatchNorm1d(hidden_layer_size)\n",
    "        self.hidden_layer_3 = nn.Linear(hidden_layer_size, hidden_layer_size // 4)\n",
    "        self.bn_3 = nn.BatchNorm1d(hidden_layer_size // 4)\n",
    "        self.cls = nn.Linear(hidden_layer_size // 4, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        bert_output = self.BERT(input_ids=input_ids, attention_mask=attention_masks)\n",
    "        hidden_out = self.hidden_layer_1(bert_output.last_hidden_state[:, 0, :])\n",
    "\n",
    "        out = self.bn_1(self.relu(hidden_out))\n",
    "        out = self.bn_2(self.relu(self.hidden_layer_2(out)))\n",
    "        out = self.bn_3(self.relu(self.hidden_layer_3(out)))\n",
    "\n",
    "        logits = self.cls(out)\n",
    "        return logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4cd8ce14",
   "metadata": {
    "cellId": "prup7f2yetnyhab5jihvl"
   },
   "outputs": [],
   "source": [
    "model = FBERTAcls(newmodel, last_layer_dim=1024, hidden_layer_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f23e8305",
   "metadata": {
    "cellId": "ko9u6dvdtag0ot4f8xuo6vf"
   },
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "11982b35",
   "metadata": {
    "cellId": "j0rb82g88yifoekop13wwh"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def training(model, optimizer, criterion, train_loader, epoch, device=\"cpu\"):\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}. Train Loss: {0}\")\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    steps = 0\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        features = batch[\"input_ids\"].to(device)\n",
    "        attn_mask = bathc(['attention_mask'].t)\n",
    "        \n",
    "        targets = batch[\"targets\"].to(device).reshape(-1, 1)\n",
    "\n",
    "        predict = model(features)\n",
    "        loss = criterion(predict, targets.to(torch.float32))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.detach().cpu()\n",
    "        \n",
    "        pbar.set_description(f\"Epoch {epoch + 1}. Train Loss: {loss.detach().cpu():.4}\")\n",
    "        steps += 1\n",
    "        \n",
    "    return total_loss / steps\n",
    "\n",
    "def testing(model, criterion, test_loader, device=\"cpu\"):\n",
    "    pbar = tqdm(test_loader, desc=f\"Test Loss: {0}, Test Acc: {0}\")\n",
    "    mean_loss = 0\n",
    "    mean_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            features = batch[\"input_ids\"].to(device)\n",
    "            targets = batch[\"targets\"].to(device).reshape(-1, 1)\n",
    "        \n",
    "            predict_probs = model(features)\n",
    "            loss = criterion(predict_probs, targets.to(torch.float32))\n",
    "\n",
    "            predict = predict_probs > 0.5\n",
    "            acc = (predict == targets).sum() / len(targets)\n",
    "\n",
    "            mean_loss += loss.item()\n",
    "            mean_acc += acc.item()\n",
    "\n",
    "            pbar.set_description(f\"Test Loss: {loss:.4}, Test Acc: {acc:.4}\")\n",
    "\n",
    "    pbar.set_description(f\"Test Loss: {mean_loss / len(test_loader):.4}, Test Acc: {mean_acc / len(test_loader):.4}\")\n",
    "\n",
    "    return {\"Test Loss\": mean_loss / len(test_loader), \"Test Acc\": mean_acc / len(test_loader)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "216b7881",
   "metadata": {
    "cellId": "365yuv1swd97r4npr95mg"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ba806868",
   "metadata": {
    "cellId": "epltmkjgtbbsoya9zmyx"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a949411956486fbbea91c005eec455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1. Train Loss: 0:   0%|          | 0/1936 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭──────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">&lt;ipython-input-29-89483fdf7b66&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">&lt;ipython-input-27-5a4c37499a59&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">14</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">training</span>                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1051</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1048 │   │   # this function, and just call forward.</span>                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1049 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_h <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1050 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1051 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1052 │   │   # Do not call functions when jit is used</span>                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1053 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1054 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">forward</span><span style=\"font-weight: bold\">()</span> missing <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> required positional argument: <span style=\"color: #008000; text-decoration-color: #008000\">'token_type_ids'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[91m╭─\u001b[0m\u001b[91m─────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[91m ───────────────────────────\u001b[0m\u001b[91m─╮\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[33m<ipython-input-29-89483fdf7b66>\u001b[0m:\u001b[94m2\u001b[0m in \u001b[92m<module>\u001b[0m                                             \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[33m<ipython-input-27-5a4c37499a59>\u001b[0m:\u001b[94m14\u001b[0m in \u001b[92mtraining\u001b[0m                                            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1051\u001b[0m in \u001b[92m_call_impl\u001b[0m      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1048 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1049 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_h \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1050 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m1051 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1052 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1053 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1054 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                         \u001b[91m│\u001b[0m\n",
       "\u001b[91m╰───────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0m\u001b[1;35mforward\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m missing \u001b[1;36m1\u001b[0m required positional argument: \u001b[32m'token_type_ids'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'token_type_ids'",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    loss = training(model, optimizer, criterion, train_loader, epoch)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fec443d3",
   "metadata": {
    "cellId": "de284nye2lq63goywvlf4i"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,  4336,  8806,  ...,     0,     0,     0],\n",
       "         [    1,  3667, 14679,  ...,     0,     0,     0],\n",
       "         [    1, 11828,   281,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [    1,  1133,  1118,  ...,     0,     0,     0],\n",
       "         [    1, 23269,   503,  ...,     0,     0,     0],\n",
       "         [    1,  2624, 46298,  ...,     0,     0,     0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'target': tensor([False, False, False,  True, False, False, False, False])}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04be565a",
   "metadata": {
    "cellId": "5ctsv6y302g8dcybk3qj2s"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "b4080bcd-8e63-4852-b615-1487758e00d2",
  "notebookPath": "hse-hackaton/baseline_model/test.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
