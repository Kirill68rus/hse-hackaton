{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97db50ab",
   "metadata": {
    "cellId": "opdtmf2zaf9h1uwa7y9q6m",
    "execution_id": "f64f0c0e-60b3-41f3-b069-02388c68ae7a"
   },
   "outputs": [],
   "source": [
    "%pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "7011c6f3",
   "metadata": {
    "cellId": "h7z1sf2ltfjil7umqhbcz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/jupyter/.local/lib/python3.8/site-packages (4.27.4)\n",
      "Requirement already satisfied: datasets in /home/jupyter/.local/lib/python3.8/site-packages (2.11.0)\n",
      "Requirement already satisfied: accelerate in /home/jupyter/.local/lib/python3.8/site-packages (0.18.0)\n",
      "Requirement already satisfied: deepspeed in /home/jupyter/.local/lib/python3.8/site-packages (0.8.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jupyter/.local/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/jupyter/.local/lib/python3.8/site-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /kernel/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: requests in /kernel/lib/python3.8/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/jupyter/.local/lib/python3.8/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2021.11.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/jupyter/.local/lib/python3.8/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: multiprocess in /home/jupyter/.local/lib/python3.8/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/jupyter/.local/lib/python3.8/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (0.25.3)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.10.15)\n",
      "Requirement already satisfied: xxhash in /kernel/fallback/lib/python3.8/site-packages (from datasets) (2.0.0)\n",
      "Requirement already satisfied: psutil in /kernel/lib/python3.8/site-packages (from accelerate) (5.7.3)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.9.1+cu111)\n",
      "Requirement already satisfied: py-cpuinfo in /home/jupyter/.local/lib/python3.8/site-packages (from deepspeed) (9.0.0)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.8/dist-packages (from deepspeed) (1.8.2)\n",
      "Requirement already satisfied: hjson in /home/jupyter/.local/lib/python3.8/site-packages (from deepspeed) (3.1.0)\n",
      "Requirement already satisfied: ninja in /home/jupyter/.local/lib/python3.8/site-packages (from deepspeed) (1.11.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /kernel/lib/python3.8/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /kernel/lib/python3.8/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /kernel/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /kernel/lib/python3.8/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: six in /kernel/lib/python3.8/site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /kernel/lib/python3.8/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /kernel/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2021.3)\n",
      "Installing collected packages: charset-normalizer\n",
      "\u001b[33m  WARNING: The script normalizer is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "moto 1.3.14 requires idna<2.9,>=2.5, but you have idna 3.4 which is incompatible.\n",
      "kaggle 1.5.8 requires urllib3<1.25,>=1.21.1, but you have urllib3 1.26.15 which is incompatible.\n",
      "cloud-ml 0.0.1 requires requests<=2.25.1,>=2.22.0, but you have requests 2.28.2 which is incompatible.\n",
      "cloud-ml 0.0.1 requires tqdm<=4.54.1,>=4.45.0, but you have tqdm 4.65.0 which is incompatible.\u001b[0m\n",
      "Successfully installed charset-normalizer-2.1.1\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "\n",
    "\n",
    "%pip install --upgrade transformers datasets accelerate deepspeed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "# import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "5bbed612",
   "metadata": {
    "cellId": "t59zouke3si9n4c7phledg"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "65093282",
   "metadata": {
    "cellId": "w7w83h7x9jml6oaewebuwc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a56b3cbdbf74a179b4d894289bf4940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/499 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39da205c7a54ddd9c6ba9a36b86147d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/943 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f465509224d419d9b36c61a70b0fd7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/1.40M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a7434b347241588c6b3f35dd4e205a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5283742b64ee4c6d9a8721accee42f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/712M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "model_name = \"Tatyana/rubert-base-cased-sentiment-new\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "1b0e2411",
   "metadata": {
    "cellId": "dum67ndv14m0i5948u4us"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "DATA_PATH = '/home/jupyter/mnt/s3/dataset_hackaton/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554d9ba9",
   "metadata": {
    "cellId": "q8wvy5j6u4phvs7f9wtyd7",
    "execution_id": "014f998b-b26e-4fb6-af7a-707d3bfbb538"
   },
   "outputs": [],
   "source": [
    "!ls /home/jupyter/mnt/s3/dataset_hackaton/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "17f4a28a",
   "metadata": {
    "cellId": "51f4ljuqgpn27aspwi5idn"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "train_df = pd.read_csv(DATA_PATH + 'train_OHE.csv')\n",
    "\n",
    "bank_categories_df = pd.read_csv(DATA_PATH + 'bank-sentiment.csv')\n",
    "bank_sentiment_df = pd.read_csv(DATA_PATH + 'bank-categories.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "1d681982",
   "metadata": {
    "cellId": "r9klh89kcmqd0c55qefgt"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>sentence</th>\n",
       "      <th>1category</th>\n",
       "      <th>2category</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4754</td>\n",
       "      <td>При этом всегда получал качественные услуги.</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4417</td>\n",
       "      <td>Не вижу, за что хотя бы 2 поставить, сервис на 1!</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3629</td>\n",
       "      <td>Вот так \"Мой любимый\" банк МКБ меня обманул.</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11640</td>\n",
       "      <td>Отвратительное отношение к клиентам.</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5571</td>\n",
       "      <td>Всегда в любое время дня и ночи помогут, ответ...</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19356</th>\n",
       "      <td>19356</td>\n",
       "      <td>8004</td>\n",
       "      <td>Никогда и ни в коем случае не открывайте счет ...</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19357</th>\n",
       "      <td>19357</td>\n",
       "      <td>18182</td>\n",
       "      <td>ТИ откровенно забили на качество и развивают с...</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19358</th>\n",
       "      <td>19358</td>\n",
       "      <td>744</td>\n",
       "      <td>Я считаю, это прорыв и лидерство финансовых ус...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19359</th>\n",
       "      <td>19359</td>\n",
       "      <td>6220</td>\n",
       "      <td>Писал мужчина очень доходчиво, не финансовым я...</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19360</th>\n",
       "      <td>19360</td>\n",
       "      <td>8433</td>\n",
       "      <td>Данная ситуация меня сильно выбила из колеи, и...</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19361 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Unnamed: 0.1  ...        2category  sentiment\n",
       "0               0          4754  ...  [0, 0, 0, 0, 0]  [1, 0, 0]\n",
       "1               1          4417  ...  [0, 0, 0, 0, 0]  [0, 0, 1]\n",
       "2               2          3629  ...  [0, 0, 0, 0, 0]  [0, 0, 1]\n",
       "3               3         11640  ...  [0, 0, 0, 0, 0]  [0, 0, 1]\n",
       "4               4          5571  ...  [0, 0, 0, 0, 0]  [1, 0, 0]\n",
       "...           ...           ...  ...              ...        ...\n",
       "19356       19356          8004  ...  [0, 0, 0, 0, 0]  [0, 0, 1]\n",
       "19357       19357         18182  ...  [0, 0, 0, 0, 0]  [0, 0, 1]\n",
       "19358       19358           744  ...  [0, 0, 0, 0, 0]  [1, 0, 0]\n",
       "19359       19359          6220  ...  [0, 0, 0, 0, 0]  [1, 0, 0]\n",
       "19360       19360          8433  ...  [0, 0, 0, 0, 0]  [0, 0, 1]\n",
       "\n",
       "[19361 rows x 6 columns]"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "33da569b",
   "metadata": {
    "cellId": "wqwlhc2kuf4vd6xnd71oi"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# indexes = np.arange(train_df.shape[0])\n",
    "# np.random.shuffle(indexes)\n",
    "# train_size = math.ceil(train_df.shape[0] * 0.8)\n",
    "\n",
    "# train_indexes = indexes[:train_size]\n",
    "# test_indexes = indexes[train_size:]\n",
    "\n",
    "# train_data = train_df.iloc[train_indexes]\n",
    "# test_data = train_df.iloc[test_indexes]\n",
    "\n",
    "# train_data.reset_index(drop=True, inplace=True)\n",
    "# test_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "indexes = np.arange(train_df.shape[0])\n",
    "np.random.shuffle(indexes)\n",
    "train_size = math.ceil(train_df.shape[0] * 0.8)\n",
    "\n",
    "data = train_df.iloc[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "44ed3ec2",
   "metadata": {
    "cellId": "mlbsdz52hltrjj3i5pcl7"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "MAX_LENGTH = 128\n",
    "def preprocess_function(examples):\n",
    "    result = tokenizer(\n",
    "        examples,\n",
    "        padding='max_length', max_length=MAX_LENGTH, truncation=True\n",
    "    )\n",
    "#     result['target'] = examples['target']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "acb132d8",
   "metadata": {
    "cellId": "0a0ym8ztqhvcisui5uoyvad"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, feature_column: str, target_columns: list, tokenizer):  \n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.feature_column = feature_column\n",
    "        self.target_columns = target_columns\n",
    "        self.label2list = lambda x: [int(i) for i in json.loads(x)]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        indx_tokens = preprocess_function(self.data[self.feature_column][item])\n",
    "        d = {}\n",
    "        for label in self.target_columns:\n",
    "            d[f'target_{label}'] = self.label2list(self.data[label][item])\n",
    "            \n",
    "        return {**indx_tokens, **d}\n",
    "\n",
    "    def get_tokens_(self, text):\n",
    "          return self.tokenizer(text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "b7455226",
   "metadata": {
    "cellId": "42bvkkbj8fx47pj56zqozx"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>sentence</th>\n",
       "      <th>1category</th>\n",
       "      <th>2category</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4754</td>\n",
       "      <td>При этом всегда получал качественные услуги.</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4417</td>\n",
       "      <td>Не вижу, за что хотя бы 2 поставить, сервис на 1!</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3629</td>\n",
       "      <td>Вот так \"Мой любимый\" банк МКБ меня обманул.</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>11640</td>\n",
       "      <td>Отвратительное отношение к клиентам.</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5571</td>\n",
       "      <td>Всегда в любое время дня и ночи помогут, ответ...</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19356</th>\n",
       "      <td>19356</td>\n",
       "      <td>8004</td>\n",
       "      <td>Никогда и ни в коем случае не открывайте счет ...</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19357</th>\n",
       "      <td>19357</td>\n",
       "      <td>18182</td>\n",
       "      <td>ТИ откровенно забили на качество и развивают с...</td>\n",
       "      <td>[0, 0, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19358</th>\n",
       "      <td>19358</td>\n",
       "      <td>744</td>\n",
       "      <td>Я считаю, это прорыв и лидерство финансовых ус...</td>\n",
       "      <td>[1, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19359</th>\n",
       "      <td>19359</td>\n",
       "      <td>6220</td>\n",
       "      <td>Писал мужчина очень доходчиво, не финансовым я...</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19360</th>\n",
       "      <td>19360</td>\n",
       "      <td>8433</td>\n",
       "      <td>Данная ситуация меня сильно выбила из колеи, и...</td>\n",
       "      <td>[0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19361 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Unnamed: 0.1  ...        2category  sentiment\n",
       "0               0          4754  ...  [0, 0, 0, 0, 0]  [1, 0, 0]\n",
       "1               1          4417  ...  [0, 0, 0, 0, 0]  [0, 0, 1]\n",
       "2               2          3629  ...  [0, 0, 0, 0, 0]  [0, 0, 1]\n",
       "3               3         11640  ...  [0, 0, 0, 0, 0]  [0, 0, 1]\n",
       "4               4          5571  ...  [0, 0, 0, 0, 0]  [1, 0, 0]\n",
       "...           ...           ...  ...              ...        ...\n",
       "19356       19356          8004  ...  [0, 0, 0, 0, 0]  [0, 0, 1]\n",
       "19357       19357         18182  ...  [0, 0, 0, 0, 0]  [0, 0, 1]\n",
       "19358       19358           744  ...  [0, 0, 0, 0, 0]  [1, 0, 0]\n",
       "19359       19359          6220  ...  [0, 0, 0, 0, 0]  [1, 0, 0]\n",
       "19360       19360          8433  ...  [0, 0, 0, 0, 0]  [0, 0, 1]\n",
       "\n",
       "[19361 rows x 6 columns]"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "d15cd7ff",
   "metadata": {
    "cellId": "604b78qb7vmd5ry63os8dn"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "dataset = Dataset(data, 'sentence', ['1category', '2category', 'sentiment'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "568989e3",
   "metadata": {
    "cellId": "mw9252evmkfch53iypnn2k"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ef06abcd",
   "metadata": {
    "cellId": "6gt8va8g32w14czuoh4lord"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4c7aa31d",
   "metadata": {
    "cellId": "5ygm6d5oixklq9aoraiqvk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5a51f464",
   "metadata": {
    "cellId": "9b8yojfk3rjzw952th3c9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "aa520fb9",
   "metadata": {
    "cellId": "k76d6nl292401u5jjfy4e"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "num_workers = 4\n",
    "\n",
    "def average_emb(batch):\n",
    "    features = [b[\"tokens\"] for b in batch]\n",
    "    targets = [b[\"target\"] for b in batch]\n",
    "\n",
    "    return {\"tokens\": features, \"targets\": torch.LongTensor(targets)}\n",
    "\n",
    "\n",
    "train_size = math.ceil(len(dataset) * 0.8)\n",
    "\n",
    "# train, valid = random_split(dataset, [train_size, len(train_data) - train_size])\n",
    "train, valid = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True, collate_fn=transformers.default_data_collator)\n",
    "valid_loader = DataLoader(valid, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, collate_fn=transformers.default_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "0c66dc40",
   "metadata": {
    "cellId": "jt4je638agbtlclo8ttvj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids',\n",
       " 'token_type_ids',\n",
       " 'attention_mask',\n",
       " 'target_1category',\n",
       " 'target_2category',\n",
       " 'target_sentiment']"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "list(next(iter(train_loader)).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "e8d2deaa",
   "metadata": {
    "cellId": "uzv81u85e6nltt7e48ljxh"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "newmodel = torch.nn.Sequential(*(list(model.children())[:-1]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "d4690fa5",
   "metadata": {
    "cellId": "74zvmn81tvuupe5aatkee8"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# newmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "12eb14a6",
   "metadata": {
    "cellId": "rseu4nzvpzr71mlrwmiw"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "21f79e39",
   "metadata": {
    "cellId": "j7yw69bx4q8dxdbk7bb5di"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "f93f84b5",
   "metadata": {
    "cellId": "h52nxu5hdbksfx2pb4zb2b"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def freeze_layers(model, num_to_freeze=0):\n",
    "    \n",
    "    for layer in list(model.parameters())[:-num_to_freeze]:\n",
    "        layer.requires_grad_ = False\n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "4c685300",
   "metadata": {
    "cellId": "p8kpzjyji7fgdaawr35705"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "\n",
    "class Cls_head(nn.Module):\n",
    "    \n",
    "    def __init__(self, last_layer_dim=1024, hidden_layer_size=512, out_size=1):\n",
    "        super().__init__()\n",
    "        self.last_layer_dim = last_layer_dim\n",
    "        self.hidden_layer_size = hidden_layer_size,\n",
    "        self.out_size = out_size\n",
    "        \n",
    "        self.hidden_layer_1 = nn.Linear(last_layer_dim, hidden_layer_size)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.bn_1 = nn.BatchNorm1d(hidden_layer_size)\n",
    "        \n",
    "        self.hidden_layer_2 = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.bn_2 = nn.BatchNorm1d(hidden_layer_size)\n",
    "        \n",
    "        \n",
    "        self.hidden_layer_3 = nn.Linear(hidden_layer_size, hidden_layer_size // 4)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.bn_3 = nn.BatchNorm1d(hidden_layer_size // 4)\n",
    "        \n",
    "        self.cls = nn.Linear(hidden_layer_size // 4, out_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.bn_1(self.relu1(self.hidden_layer_1(x)))\n",
    "        out = self.bn_2(self.relu2(self.hidden_layer_2(out)))\n",
    "        out = self.bn_3(self.relu3(self.hidden_layer_3(out)))\n",
    "        out = self.cls(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "\n",
    "class FBERTAcls(nn.Module):\n",
    "\n",
    "    def __init__(self, model, last_layer_dim=768, hidden_layer_size=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.BERT = freeze_layers(model, 0)\n",
    "        \n",
    "        self.cls_head_1 = Cls_head(last_layer_dim=last_layer_dim, hidden_layer_size=hidden_layer_size, out_size=3)\n",
    "        self.cls_head_2 = Cls_head(last_layer_dim=last_layer_dim, hidden_layer_size=hidden_layer_size, out_size=5)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        bert_output = self.BERT(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         hidden_out = self.hidden_layer_1(bert_output.last_hidden_state[:, 0, :])\n",
    "\n",
    "#         print(bert_output.shape)\n",
    "\n",
    "        bert_output = bert_output.last_hidden_state[:, 0, :]\n",
    "#         print(bert_output.shape)\n",
    "        print(bert_output.sum())\n",
    "        sent_logits = self.cls_head_1(bert_output) \n",
    "        \n",
    "        cat_1_logits = self.cls_head_2(bert_output)\n",
    "\n",
    "        return sent_logits, cat_1_logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "fdaddbb0",
   "metadata": {
    "cellId": "prup7f2yetnyhab5jihvl"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model = FBERTAcls(newmodel, last_layer_dim=768, hidden_layer_size=1024).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "63ad43d9",
   "metadata": {
    "cellId": "jwswknvz7lh7tasv98gdo"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "d5f4afa6",
   "metadata": {
    "cellId": "aief3lhet5c0rws46okdpvh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids',\n",
       " 'token_type_ids',\n",
       " 'attention_mask',\n",
       " 'target_1category',\n",
       " 'target_2category',\n",
       " 'target_sentiment']"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "batch = next(iter(train_loader))\n",
    "list(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "45a620e2",
   "metadata": {
    "cellId": "ko9u6dvdtag0ot4f8xuo6vf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-156.4062, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.0024,  0.2347, -0.3949],\n",
       "         [ 0.3470, -0.1218, -0.8285],\n",
       "         [-0.5675, -0.2159,  0.3164],\n",
       "         [ 0.6167, -0.5360, -1.0351],\n",
       "         [ 0.2754, -0.2067,  0.5946],\n",
       "         [ 0.2642,  0.1652,  1.3120],\n",
       "         [ 0.0483,  0.7264,  0.4687],\n",
       "         [-0.1964,  0.4268, -0.5441]], grad_fn=<AddmmBackward>),\n",
       " tensor([[-0.3497, -0.3287, -0.4689,  1.1853, -1.5885],\n",
       "         [ 0.5427, -0.5132, -0.3485,  0.0564,  0.0781],\n",
       "         [-0.2073,  1.1617, -0.0938, -0.2017,  0.1136],\n",
       "         [-0.1618,  0.6219,  0.3582, -0.5494,  0.0696],\n",
       "         [ 0.5467, -0.1068, -0.4584, -0.9157,  1.3640],\n",
       "         [ 0.1283, -0.4869,  0.1815,  0.7330,  0.5572],\n",
       "         [-0.5337, -0.3652, -0.2505,  0.1101, -0.7109],\n",
       "         [-0.3056,  0.4062,  0.7611, -0.7672,  0.4678]],\n",
       "        grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "model(batch['input_ids'], batch['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "2f1f3824",
   "metadata": {
    "cellId": "z92o2vzjhphtjqbxzq98sg"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def compute_loss(logits_sent, logits_1cat, targets_sent, targets_1cat):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    loss = ce_loss(logits_sent, targets_sent) + 0.5 * ce_loss(logits_1cat, targets_1cat)\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "8bfee5f6",
   "metadata": {
    "cellId": "j0rb82g88yifoekop13wwh"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# ['input_ids',\n",
    "#  'token_type_ids',\n",
    "#  'attention_mask',\n",
    "#  'target_1category',\n",
    "#  'target_2category',\n",
    "#  'target_sentiment']\n",
    "\n",
    "\n",
    "def training(model, optimizer, criterion, train_loader, epoch, device=\"cpu\"):\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}. Train Loss: {0}\")\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    steps = 0\n",
    "    \n",
    "    count = 0\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        features = batch[\"input_ids\"].to(device)\n",
    "        attn_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        targets_sent = torch.argmax(batch[\"target_sentiment\"].to(device), dim=-1)\n",
    "        targets_1cat = torch.argmax(batch[\"target_1category\"].to(device), dim=-1)\n",
    "\n",
    "        sent_logits, cat_logits = model(features, attn_mask)\n",
    "#         print(sent_logits)\n",
    "        \n",
    "        loss = compute_loss(sent_logits, cat_logits, targets_sent, targets_1cat)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.detach().cpu()\n",
    "        \n",
    "        pbar.set_description(f\"Epoch {epoch + 1}. Train Loss: {loss.detach().cpu():.4}\")\n",
    "        steps += 1\n",
    "        \n",
    "    return total_loss / steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "64402dd9",
   "metadata": {
    "cellId": "n8l943vlw88uawdn7ykg4"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def testing(model, criterion, test_loader, threshold=0.5, device=\"cpu\"):\n",
    "    pbar = tqdm(test_loader, desc=f\"Test Loss: {0}, Test Acc: {0}\")\n",
    "    mean_loss = 0\n",
    "    mean_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    all_probs_sent = []\n",
    "    all_probs_1cat = []\n",
    "    \n",
    "    target_labels_sent = []\n",
    "    target_labels_sent_1cat = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            \n",
    "            features = batch[\"input_ids\"].to(device)\n",
    "            attn_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "            targets_sent = torch.argmax(batch[\"target_sentiment\"].to(device), dim=-1)\n",
    "            targets_1cat = torch.argmax(batch[\"target_1category\"].to(device), dim=-1)\n",
    "\n",
    "            sent_logits, cat_logits = model(features, attn_mask)\n",
    "            print(sent_logits)\n",
    "            \n",
    "            target_labels_sent.append(batch[\"target_sentiment\"].cpu().numpy())\n",
    "            target_labels_sent_1cat.append(batch[\"target_sentiment\"].cpu().numpy())\n",
    "            \n",
    "            all_probs_sent.append(torch.softmax(sent_logits, dim=-1).cpu().numpy())\n",
    "            all_probs_1cat.append(torch.softmax(cat_logits, dim=-1).cpu().numpy())\n",
    "            \n",
    "            loss = compute_loss(sent_logits, cat_logits, targets_sent, targets_1cat)\n",
    "\n",
    "#             predict = probas.cpu() > threshold\n",
    "#             acc = (predict.cpu() == targets.cpu()).sum() / len(targets)\n",
    "\n",
    "            mean_loss += loss.item()\n",
    "#             mean_acc += acc.item()\n",
    "\n",
    "#             pbar.set_description(f\"Test Loss: {loss:.4}, Test Acc: {acc:.4}\")\n",
    "\n",
    "#     pbar.set_description(f\"Test Loss: {mean_loss / len(test_loader):.4}, Test Acc: {mean_acc / len(test_loader):.4}\")\n",
    "    return {'all_probs_sent': all_probs_sent,\n",
    "            'all_probs_1cat': all_probs_1cat, \n",
    "           'target_sent': target_labels_sent,\n",
    "           'target_1cat': target_labels_sent_1cat}\n",
    "#     return {\"Test Loss\": mean_loss / len(test_loader), \"Test Acc\": mean_acc / len(test_loader), 'target_labels': target_labels, 'probs': all_probs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "08ac9dfc",
   "metadata": {
    "cellId": "365yuv1swd97r4npr95mg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "a0a7e060",
   "metadata": {
    "cellId": "s28idusgmaiphgw1nbcws"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "56f37221",
   "metadata": {
    "cellId": "epltmkjgtbbsoya9zmyx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f319630b744cff9fa07ad335353d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1. Train Loss: 0:   0%|          | 0/1936 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-156.6514, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-156.1871, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-159.8499, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-159.8631, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-163.8573, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-160.0812, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-162.6152, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-158.8139, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-159.7565, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-159.5452, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(-154.8857, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭──────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">&lt;ipython-input-110-6edc8f719c6e&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">&lt;ipython-input-106-a53203919001&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">34</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">training</span>                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">255</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 252 │   │   │   │   </span>retain_graph=retain_graph,                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 253 │   │   │   │   </span>create_graph=create_graph,                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 254 │   │   │   │   </span>inputs=inputs)                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 255 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 256 │   </span>                                                                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 257 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">register_hook</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, hook):                                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 258 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">r\"\"\"Registers a backward hook.</span>                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">147</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">144 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> retain_graph <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">145 │   │   </span>retain_graph = create_graph                                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">146 │   </span>                                                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>147 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>Variable._execution_engine.run_backward(                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">148 │   │   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">149 │   │   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># allow_unreachable flag</span>     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">150 </span>                                                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[91m╭─\u001b[0m\u001b[91m─────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[91m ───────────────────────────\u001b[0m\u001b[91m─╮\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[33m<ipython-input-110-6edc8f719c6e>\u001b[0m:\u001b[94m5\u001b[0m in \u001b[92m<module>\u001b[0m                                            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[33m<ipython-input-106-a53203919001>\u001b[0m:\u001b[94m34\u001b[0m in \u001b[92mtraining\u001b[0m                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m255\u001b[0m in \u001b[92mbackward\u001b[0m                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 252 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mretain_graph=retain_graph,                                         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 253 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                                         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 254 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs)                                                     \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m 255 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 256 \u001b[0m\u001b[2m│   \u001b[0m                                                                               \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 257 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mregister_hook\u001b[0m(\u001b[96mself\u001b[0m, hook):                                                 \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 258 \u001b[0m\u001b[2;90m│   │   \u001b[0m\u001b[33mr\u001b[0m\u001b[33m\"\"\"Registers a backward hook.\u001b[0m                                             \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m147\u001b[0m in \u001b[92mbackward\u001b[0m         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m144 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m retain_graph \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                        \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m145 \u001b[0m\u001b[2m│   │   \u001b[0mretain_graph = create_graph                                                 \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m146 \u001b[0m\u001b[2m│   \u001b[0m                                                                                \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m147 \u001b[2m│   \u001b[0mVariable._execution_engine.run_backward(                                        \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m148 \u001b[0m\u001b[2m│   │   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                 \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m149 \u001b[0m\u001b[2m│   │   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# allow_unreachable flag\u001b[0m     \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m150 \u001b[0m                                                                                    \u001b[91m│\u001b[0m\n",
       "\u001b[91m╰───────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "losses = []\n",
    "print(device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    loss = training(model, optimizer, criterion, train_loader, epoch, device)\n",
    "    losses.append(loss)\n",
    "    plt.plot(list(range(epoch + 1)), losses)\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "563e261a",
   "metadata": {
    "cellId": "de284nye2lq63goywvlf4i"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b4ab601d8c4352866edf01d00e455d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Loss: 0, Test Acc: 0:   0%|          | 0/1936 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n",
      "tensor(-159.6098, device='cuda:0')\n",
      "tensor([[ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551],\n",
      "        [ 0.0002, -0.0375,  0.0551]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭──────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">&lt;ipython-input-111-f9b0aff076c7&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">&lt;ipython-input-107-5352bcc6f27d&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">23</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">testing</span>                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1051</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1048 │   │   # this function, and just call forward.</span>                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1049 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_h <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1050 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1051 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1052 │   │   # Do not call functions when jit is used</span>                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1053 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1054 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">&lt;ipython-input-100-93fd3087c27c&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">51</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1051</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1048 │   │   # this function, and just call forward.</span>                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1049 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_h <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1050 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1051 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1052 │   │   # Do not call functions when jit is used</span>                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1053 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1054 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jupyter/.local/lib/python3.8/site-packages/transformers/models/bert/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_bert.p</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">y</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1020</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1017 │   │   │   </span>inputs_embeds=inputs_embeds,                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1018 │   │   │   </span>past_key_values_length=past_key_values_length,                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1019 │   │   </span>)                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1020 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>encoder_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.encoder(                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1021 │   │   │   </span>embedding_output,                                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1022 │   │   │   </span>attention_mask=extended_attention_mask,                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1023 │   │   │   </span>head_mask=head_mask,                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1051</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1048 │   │   # this function, and just call forward.</span>                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1049 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_h <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1050 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1051 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1052 │   │   # Do not call functions when jit is used</span>                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1053 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1054 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jupyter/.local/lib/python3.8/site-packages/transformers/models/bert/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_bert.p</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">y</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">610</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 607 │   │   │   │   │   </span>encoder_attention_mask,                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 608 │   │   │   │   </span>)                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 609 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 610 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>layer_outputs = layer_module(                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 611 │   │   │   │   │   </span>hidden_states,                                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 612 │   │   │   │   │   </span>attention_mask,                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 613 │   │   │   │   │   </span>layer_head_mask,                                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1051</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1048 │   │   # this function, and just call forward.</span>                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1049 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_h <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1050 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1051 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1052 │   │   # Do not call functions when jit is used</span>                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1053 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1054 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jupyter/.local/lib/python3.8/site-packages/transformers/models/bert/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_bert.p</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">y</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">495</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 492 │   </span>) -&gt; Tuple[torch.Tensor]:                                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 493 │   │   # decoder uni-directional self-attention cached key/values tuple is at pos</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 494 │   │   </span>self_attn_past_key_value = past_key_value[:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>] <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> past_key_value <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">Non</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 495 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>self_attention_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.attention(                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 496 │   │   │   </span>hidden_states,                                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 497 │   │   │   </span>attention_mask,                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 498 │   │   │   </span>head_mask,                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1051</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1048 │   │   # this function, and just call forward.</span>                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1049 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_h <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1050 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1051 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1052 │   │   # Do not call functions when jit is used</span>                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1053 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1054 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jupyter/.local/lib/python3.8/site-packages/transformers/models/bert/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_bert.p</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">y</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">425</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 422 │   │   </span>past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>,          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 423 │   │   </span>output_attentions: Optional[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">bool</span>] = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>,                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 424 │   </span>) -&gt; Tuple[torch.Tensor]:                                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 425 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>self_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.self(                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 426 │   │   │   </span>hidden_states,                                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 427 │   │   │   </span>attention_mask,                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 428 │   │   │   </span>head_mask,                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1051</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1048 │   │   # this function, and just call forward.</span>                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1049 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_h <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1050 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1051 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1052 │   │   # Do not call functions when jit is used</span>                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1053 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1054 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/jupyter/.local/lib/python3.8/site-packages/transformers/models/bert/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_bert.p</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">y</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">306</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 303 │   │   │   </span>key_layer = torch.cat([past_key_value[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>], key_layer], dim=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>)           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 304 │   │   │   </span>value_layer = torch.cat([past_key_value[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>], value_layer], dim=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>)       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 305 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 306 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>key_layer = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.transpose_for_scores(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.key(hidden_states))         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 307 │   │   │   </span>value_layer = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.transpose_for_scores(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.value(hidden_states))     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 308 │   │   </span>                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 309 │   │   </span>query_layer = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.transpose_for_scores(mixed_query_layer)                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1051</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1048 │   │   # this function, and just call forward.</span>                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1049 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_h <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1050 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1051 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1052 │   │   # Do not call functions when jit is used</span>                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1053 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1054 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">linear.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">96</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 93 │   │   │   </span>init.uniform_(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bias, -bound, bound)                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 94 │   </span>                                                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 95 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>: Tensor) -&gt; Tensor:                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 96 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> F.linear(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.weight, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bias)                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 97 │   </span>                                                                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 98 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">extra_repr</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>) -&gt; <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>:                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 99 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #808000; text-decoration-color: #808000\">'in_features={}, out_features={}, bias={}'</span>.format(                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/torch/nn/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">functional.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1847</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">linear</span>              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1844 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1845 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> has_torch_function_variadic(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, weight):                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1846 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> handle_torch_function(linear, (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, weight), <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, weight, bias= <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1847 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> torch._C._nn.linear(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, weight, bias)                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1848 </span>                                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1849 </span>                                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1850 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">bilinear</span>(input1: Tensor, input2: Tensor, weight: Tensor, bias: Optional[Tensor <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[91m╭─\u001b[0m\u001b[91m─────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[91m ───────────────────────────\u001b[0m\u001b[91m─╮\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[33m<ipython-input-111-f9b0aff076c7>\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<module>\u001b[0m                                            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[33m<ipython-input-107-5352bcc6f27d>\u001b[0m:\u001b[94m23\u001b[0m in \u001b[92mtesting\u001b[0m                                            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1051\u001b[0m in \u001b[92m_call_impl\u001b[0m      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1048 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1049 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_h \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1050 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m1051 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1052 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1053 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1054 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[33m<ipython-input-100-93fd3087c27c>\u001b[0m:\u001b[94m51\u001b[0m in \u001b[92mforward\u001b[0m                                            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1051\u001b[0m in \u001b[92m_call_impl\u001b[0m      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1048 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1049 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_h \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1050 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m1051 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1052 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1053 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1054 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/home/jupyter/.local/lib/python3.8/site-packages/transformers/models/bert/\u001b[0m\u001b[1;33mmodeling_bert.p\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[1;33my\u001b[0m:\u001b[94m1020\u001b[0m in \u001b[92mforward\u001b[0m                                                                         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1017 \u001b[0m\u001b[2m│   │   │   \u001b[0minputs_embeds=inputs_embeds,                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1018 \u001b[0m\u001b[2m│   │   │   \u001b[0mpast_key_values_length=past_key_values_length,                         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1019 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                          \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m1020 \u001b[2m│   │   \u001b[0mencoder_outputs = \u001b[96mself\u001b[0m.encoder(                                            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1021 \u001b[0m\u001b[2m│   │   │   \u001b[0membedding_output,                                                      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1022 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=extended_attention_mask,                                \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1023 \u001b[0m\u001b[2m│   │   │   \u001b[0mhead_mask=head_mask,                                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1051\u001b[0m in \u001b[92m_call_impl\u001b[0m      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1048 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1049 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_h \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1050 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m1051 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1052 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1053 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1054 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/home/jupyter/.local/lib/python3.8/site-packages/transformers/models/bert/\u001b[0m\u001b[1;33mmodeling_bert.p\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[1;33my\u001b[0m:\u001b[94m610\u001b[0m in \u001b[92mforward\u001b[0m                                                                          \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 607 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mencoder_attention_mask,                                        \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 608 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 609 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m 610 \u001b[2m│   │   │   │   \u001b[0mlayer_outputs = layer_module(                                      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 611 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mhidden_states,                                                 \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 612 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mattention_mask,                                                \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 613 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mlayer_head_mask,                                               \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1051\u001b[0m in \u001b[92m_call_impl\u001b[0m      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1048 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1049 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_h \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1050 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m1051 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1052 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1053 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1054 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/home/jupyter/.local/lib/python3.8/site-packages/transformers/models/bert/\u001b[0m\u001b[1;33mmodeling_bert.p\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[1;33my\u001b[0m:\u001b[94m495\u001b[0m in \u001b[92mforward\u001b[0m                                                                          \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 492 \u001b[0m\u001b[2m│   \u001b[0m) -> Tuple[torch.Tensor]:                                                      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 493 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# decoder uni-directional self-attention cached key/values tuple is at pos\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 494 \u001b[0m\u001b[2m│   │   \u001b[0mself_attn_past_key_value = past_key_value[:\u001b[94m2\u001b[0m] \u001b[94mif\u001b[0m past_key_value \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNon\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m 495 \u001b[2m│   │   \u001b[0mself_attention_outputs = \u001b[96mself\u001b[0m.attention(                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 496 \u001b[0m\u001b[2m│   │   │   \u001b[0mhidden_states,                                                         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 497 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask,                                                        \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 498 \u001b[0m\u001b[2m│   │   │   \u001b[0mhead_mask,                                                             \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1051\u001b[0m in \u001b[92m_call_impl\u001b[0m      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1048 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1049 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_h \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1050 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m1051 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1052 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1053 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1054 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/home/jupyter/.local/lib/python3.8/site-packages/transformers/models/bert/\u001b[0m\u001b[1;33mmodeling_bert.p\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[1;33my\u001b[0m:\u001b[94m425\u001b[0m in \u001b[92mforward\u001b[0m                                                                          \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 422 \u001b[0m\u001b[2m│   │   \u001b[0mpast_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = \u001b[94mNone\u001b[0m,          \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 423 \u001b[0m\u001b[2m│   │   \u001b[0moutput_attentions: Optional[\u001b[96mbool\u001b[0m] = \u001b[94mFalse\u001b[0m,                                 \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 424 \u001b[0m\u001b[2m│   \u001b[0m) -> Tuple[torch.Tensor]:                                                      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m 425 \u001b[2m│   │   \u001b[0mself_outputs = \u001b[96mself\u001b[0m.self(                                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 426 \u001b[0m\u001b[2m│   │   │   \u001b[0mhidden_states,                                                         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 427 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask,                                                        \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 428 \u001b[0m\u001b[2m│   │   │   \u001b[0mhead_mask,                                                             \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1051\u001b[0m in \u001b[92m_call_impl\u001b[0m      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1048 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1049 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_h \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1050 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m1051 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1052 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1053 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1054 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/home/jupyter/.local/lib/python3.8/site-packages/transformers/models/bert/\u001b[0m\u001b[1;33mmodeling_bert.p\u001b[0m \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[1;33my\u001b[0m:\u001b[94m306\u001b[0m in \u001b[92mforward\u001b[0m                                                                          \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 303 \u001b[0m\u001b[2m│   │   │   \u001b[0mkey_layer = torch.cat([past_key_value[\u001b[94m0\u001b[0m], key_layer], dim=\u001b[94m2\u001b[0m)           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 304 \u001b[0m\u001b[2m│   │   │   \u001b[0mvalue_layer = torch.cat([past_key_value[\u001b[94m1\u001b[0m], value_layer], dim=\u001b[94m2\u001b[0m)       \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 305 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m 306 \u001b[2m│   │   │   \u001b[0mkey_layer = \u001b[96mself\u001b[0m.transpose_for_scores(\u001b[96mself\u001b[0m.key(hidden_states))         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 307 \u001b[0m\u001b[2m│   │   │   \u001b[0mvalue_layer = \u001b[96mself\u001b[0m.transpose_for_scores(\u001b[96mself\u001b[0m.value(hidden_states))     \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 308 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 309 \u001b[0m\u001b[2m│   │   \u001b[0mquery_layer = \u001b[96mself\u001b[0m.transpose_for_scores(mixed_query_layer)                 \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1051\u001b[0m in \u001b[92m_call_impl\u001b[0m      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1048 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1049 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_h \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1050 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m1051 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                  \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1052 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1053 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                      \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1054 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                         \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mlinear.py\u001b[0m:\u001b[94m96\u001b[0m in \u001b[92mforward\u001b[0m           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 93 \u001b[0m\u001b[2m│   │   │   \u001b[0minit.uniform_(\u001b[96mself\u001b[0m.bias, -bound, bound)                                 \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 94 \u001b[0m\u001b[2m│   \u001b[0m                                                                                \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 95 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, \u001b[96minput\u001b[0m: Tensor) -> Tensor:                                     \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m 96 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m F.linear(\u001b[96minput\u001b[0m, \u001b[96mself\u001b[0m.weight, \u001b[96mself\u001b[0m.bias)                              \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 97 \u001b[0m\u001b[2m│   \u001b[0m                                                                                \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 98 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mextra_repr\u001b[0m(\u001b[96mself\u001b[0m) -> \u001b[96mstr\u001b[0m:                                                    \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m 99 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[33m'\u001b[0m\u001b[33min_features=\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m, out_features=\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m, bias=\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m'\u001b[0m.format(                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/nn/\u001b[0m\u001b[1;33mfunctional.py\u001b[0m:\u001b[94m1847\u001b[0m in \u001b[92mlinear\u001b[0m              \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m                                                                                           \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1844 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                            \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1845 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m has_torch_function_variadic(\u001b[96minput\u001b[0m, weight):                                 \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1846 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m handle_torch_function(linear, (\u001b[96minput\u001b[0m, weight), \u001b[96minput\u001b[0m, weight, bias= \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m \u001b[31m❱ \u001b[0m1847 \u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m torch._C._nn.linear(\u001b[96minput\u001b[0m, weight, bias)                                \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1848 \u001b[0m                                                                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1849 \u001b[0m                                                                                   \u001b[91m│\u001b[0m\n",
       "\u001b[91m│\u001b[0m   \u001b[2m1850 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mbilinear\u001b[0m(input1: Tensor, input2: Tensor, weight: Tensor, bias: Optional[Tensor \u001b[91m│\u001b[0m\n",
       "\u001b[91m╰───────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "t  = testing(model, criterion, train_loader, 0.5, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "c41a75e4",
   "metadata": {
    "cellId": "5ctsv6y302g8dcybk3qj2s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all_probs_sent', 'all_probs_1cat', 'target_sent', 'target_1cat']"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "list(t.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "f0be8c79",
   "metadata": {
    "cellId": "41gyvy1v6as2w2r24958ca"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "all_probs_sent = t['all_probs_sent']\n",
    "all_probs_1cat = t['all_probs_1cat']\n",
    "target_sent = t['target_sent']\n",
    "target_1cat = t['target_1cat']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "ca2b0049",
   "metadata": {
    "cellId": "w2a8y7n0bhfhjjfa8we3qb"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "250518e3",
   "metadata": {
    "cellId": "anp238q26rr87o19261nud"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf44cd2",
   "metadata": {
    "cellId": "76v23582ifdgy4o8u1gcl"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e19bd6",
   "metadata": {
    "cellId": "8zdvfwqe0krf9huiysawcn"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cefd0bd",
   "metadata": {
    "cellId": "6w10e5c7qjgvnzfmhtqhd"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "b4080bcd-8e63-4852-b615-1487758e00d2",
  "notebookPath": "hse-hackaton/baseline_model/test.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
