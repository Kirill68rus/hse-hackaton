{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561c3c93",
   "metadata": {
    "cellId": "3u1beqrp0h8jf03p9lu48"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "#model 1\n",
    "\n",
    "model_name = \"Tatyana/rubert-base-cased-sentiment-new\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "newmodel = torch.nn.Sequential(*(list(model.children())[:-1]))[0]\n",
    "\n",
    "#!g1.1\n",
    "\n",
    "\n",
    "class Cls_head(nn.Module):\n",
    "    \n",
    "    def __init__(self, last_layer_dim=1024, hidden_layer_size=512, out_size=1):\n",
    "        super().__init__()\n",
    "        self.last_layer_dim = last_layer_dim\n",
    "        self.hidden_layer_size = hidden_layer_size,\n",
    "        self.out_size = out_size\n",
    "        \n",
    "        self.hidden_layer_1 = nn.Linear(last_layer_dim, hidden_layer_size)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.bn_1 = nn.BatchNorm1d(hidden_layer_size)\n",
    "        \n",
    "        self.hidden_layer_2 = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "#         self.bn_2 = nn.BatchNorm1d(hidden_layer_size)\n",
    "        self.dropout2 = nn.Dropout(p=0.3)\n",
    "        \n",
    "        \n",
    "        self.hidden_layer_3 = nn.Linear(hidden_layer_size, hidden_layer_size // 4)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.bn_3 = nn.BatchNorm1d(hidden_layer_size // 4)\n",
    "        \n",
    "        self.cls = nn.Linear(hidden_layer_size // 4, out_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.bn_1(self.relu1(self.hidden_layer_1(x)))\n",
    "#         out = self.bn_2(self.relu2(self.hidden_layer_2(out)))\n",
    "        out = self.dropout2(self.relu2(self.hidden_layer_2(out)))\n",
    "        out = self.bn_3(self.relu3(self.hidden_layer_3(out)))\n",
    "        out = self.cls(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "\n",
    "class FBERTAcls(nn.Module):\n",
    "\n",
    "    def __init__(self, model, last_layer_dim=768, hidden_layer_size=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.BERT = freeze_layers(model, 5)\n",
    "        \n",
    "        self.cls_head_1 = Cls_head(last_layer_dim=last_layer_dim, hidden_layer_size=hidden_layer_size, out_size=3)\n",
    "        self.cls_head_2 = Cls_head(last_layer_dim=last_layer_dim, hidden_layer_size=hidden_layer_size, out_size=5)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        bert_output = self.BERT(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         hidden_out = self.hidden_layer_1(bert_output.last_hidden_state[:, 0, :])\n",
    "\n",
    "#         print(bert_output.shape)\n",
    "\n",
    "        bert_output = bert_output.pooler_output\n",
    "#         print(bert_output.shape)\n",
    "#         print(bert_output.sum())\n",
    "        sent_logits = self.cls_head_1(bert_output) \n",
    "        \n",
    "        cat_1_logits = self.cls_head_2(bert_output)\n",
    "\n",
    "        return sent_logits, cat_1_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba96467",
   "metadata": {
    "cellId": "d67wyra211ezzcbjxyecei"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "#!g1.1\n",
    "def freeze_layers(model, num_to_freeze=0):\n",
    "    \n",
    "    for layer in list(model.parameters())[:-num_to_freeze]:\n",
    "        layer.requires_grad = False\n",
    "#         print(layer.requires_grad)\n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c45a458",
   "metadata": {
    "cellId": "sdyw43ahlg4juzs6u2uia"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "#model 2\n",
    "\n",
    "model_name = \"cointegrated/rubert-tiny\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "base_model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "newmodel = torch.nn.Sequential(*(list(base_model.children())[:-1]))[0]\n",
    "\n",
    "class Cls_head(nn.Module):\n",
    "    \n",
    "    def __init__(self, last_layer_dim=1024, hidden_layer_size=512, out_size=1):\n",
    "        super().__init__()\n",
    "        self.last_layer_dim = last_layer_dim\n",
    "        self.hidden_layer_size = hidden_layer_size,\n",
    "        self.out_size = out_size\n",
    "        \n",
    "        self.hidden_layer_1 = nn.Linear(last_layer_dim, hidden_layer_size)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.bn_1 = nn.BatchNorm1d(hidden_layer_size)\n",
    "        \n",
    "        self.hidden_layer_2 = nn.Linear(hidden_layer_size, hidden_layer_size)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "#         self.bn_2 = nn.BatchNorm1d(hidden_layer_size)\n",
    "        self.dropout2 = nn.Dropout(p=0.3)\n",
    "        \n",
    "        \n",
    "        self.hidden_layer_3 = nn.Linear(hidden_layer_size, hidden_layer_size // 4)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.bn_3 = nn.BatchNorm1d(hidden_layer_size // 4)\n",
    "        \n",
    "        self.cls = nn.Linear(hidden_layer_size // 4, out_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.bn_1(self.relu1(self.hidden_layer_1(x)))\n",
    "#         out = self.bn_2(self.relu2(self.hidden_layer_2(out)))\n",
    "        out = self.dropout2(self.relu2(self.hidden_layer_2(out)))\n",
    "        out = self.bn_3(self.relu3(self.hidden_layer_3(out)))\n",
    "        out = self.cls(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "\n",
    "class FBERTAcls(nn.Module):\n",
    "\n",
    "    def __init__(self, model, last_layer_dim=768, hidden_layer_size=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.BERT = freeze_layers(model, 1)\n",
    "        \n",
    "        self.cls_head_1 = Cls_head(last_layer_dim=last_layer_dim, hidden_layer_size=hidden_layer_size, out_size=3)\n",
    "        self.cls_head_2 = Cls_head(last_layer_dim=last_layer_dim, hidden_layer_size=hidden_layer_size, out_size=5)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        bert_output = self.BERT(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         hidden_out = self.hidden_layer_1(bert_output.last_hidden_state[:, 0, :])\n",
    "\n",
    "#         print(bert_output.shape)\n",
    "\n",
    "        bert_output = bert_output.pooler_output\n",
    "#         print(bert_output.shape)\n",
    "#         print(bert_output.sum())\n",
    "        sent_logits = self.cls_head_1(bert_output) \n",
    "        \n",
    "        cat_1_logits = self.cls_head_2(bert_output)\n",
    "\n",
    "        return sent_logits, cat_1_logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80b2ba99",
   "metadata": {
    "cellId": "ehxfnre10tfz7esh2qb5x"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# model_3 the same as 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8bb440",
   "metadata": {
    "cellId": "j88204z0du3y0mwpmasp6"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "9bb21881-6133-4858-8cf3-5e94e094fd21",
  "notebookPath": "hse-hackaton/baseline_model/models.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
